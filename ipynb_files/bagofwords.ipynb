{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfbf57e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import the necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd16997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## User defined dataset (Two simple documents containing one sentence each)\n",
    "\n",
    "documentA = 'the man went out for a walk'\n",
    "documentB = 'the children sat around the fire'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9e4c0e",
   "metadata": {},
   "source": [
    "# BOW (to convert text into vectors of numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd6a5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'man', 'went', 'out', 'for', 'a', 'walk']\n",
      "['the', 'children', 'sat', 'around', 'the', 'fire']\n"
     ]
    }
   ],
   "source": [
    "# splits two documents in individual words\n",
    "\n",
    "bagOfWordsA = documentA.split(' ')\n",
    "bagOfWordsB = documentB.split(' ')\n",
    "\n",
    "print(bagOfWordsA)\n",
    "print(bagOfWordsB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d19f6ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any duplicate words\n",
    "\n",
    "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b73b110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'for', 'a', 'around', 'the', 'walk', 'sat', 'went', 'out', 'man', 'fire', 'children'}\n"
     ]
    }
   ],
   "source": [
    "print(uniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70baca8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   for  a  around  the  walk  sat  went  out  man  fire  children\n",
      "0    1  1       0    1     1    0     1    1    1     0         0\n",
      "1    0  0       1    2     0    1     0    0    0     1         1\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary of words and their occurence for each document in the corpus (collection of documents)\n",
    "\n",
    "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "\n",
    "for word in bagOfWordsA:\n",
    "    numOfWordsA[word] += 1\n",
    "        \n",
    "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
    "\n",
    "for word in bagOfWordsB:\n",
    "    numOfWordsB[word] += 1\n",
    "    \n",
    "df = pd.DataFrame([numOfWordsA, numOfWordsB])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "661d174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Term Frequency (TF)\n",
    "\n",
    "\n",
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8882eb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'for': 0.14285714285714285, 'a': 0.14285714285714285, 'around': 0.0, 'the': 0.14285714285714285, 'walk': 0.14285714285714285, 'sat': 0.0, 'went': 0.14285714285714285, 'out': 0.14285714285714285, 'man': 0.14285714285714285, 'fire': 0.0, 'children': 0.0}\n",
      "{'for': 0.0, 'a': 0.0, 'around': 0.16666666666666666, 'the': 0.3333333333333333, 'walk': 0.0, 'sat': 0.16666666666666666, 'went': 0.0, 'out': 0.0, 'man': 0.0, 'fire': 0.16666666666666666, 'children': 0.16666666666666666}\n"
     ]
    }
   ],
   "source": [
    "# compute the term frequency for each of documents\n",
    "\n",
    "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
    "tfB = computeTF(numOfWordsB, bagOfWordsB)\n",
    "print(tfA)\n",
    "print(tfB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c471adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse Data Frequency (IDF)\n",
    "\n",
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d43619c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        for         a    around  the      walk       sat      went       out  \\\n",
      "0  0.693147  0.693147  0.693147  0.0  0.693147  0.693147  0.693147  0.693147   \n",
      "\n",
      "        man      fire  children  \n",
      "0  0.693147  0.693147  0.693147  \n"
     ]
    }
   ],
   "source": [
    "# The IDF is computed once for all documents\n",
    "\n",
    "idfs = computeIDF([numOfWordsA, numOfWordsB])\n",
    "df = pd.DataFrame([idfs])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc25070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The TF-IDF is simply the TF multiplied by IDF\n",
    "\n",
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13cc0616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        for         a    around  the      walk       sat      went       out  \\\n",
      "0  0.099021  0.099021  0.000000  0.0  0.099021  0.000000  0.099021  0.099021   \n",
      "1  0.000000  0.000000  0.115525  0.0  0.000000  0.115525  0.000000  0.000000   \n",
      "\n",
      "        man      fire  children  \n",
      "0  0.099021  0.000000  0.000000  \n",
      "1  0.000000  0.115525  0.115525  \n"
     ]
    }
   ],
   "source": [
    "# compute the TF-IDF scores for all the words in the corpus\n",
    "\n",
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "df = pd.DataFrame([tfidfA, tfidfB])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2c34ed",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf5c529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "  \n",
    "# execute the text here as :\n",
    "text = \"\"\"Beans. I was trying to explain to somebody as we were flying in,that’s corn. That’s beans. And they were very impressed at my agricultural knowledge. Please give it up for Amaury once again for that outstanding introduction. I have a bunch of good friends here today, including somebody who I served with, who is one of the finest senators in the country, and we’re lucky to have him, your Senator, Dick Durbin is here. I also noticed, by the way, former Governor Edgar here, who I haven’t seen in a long time, and somehow he has not aged and I have. And it’s great to see you, Governor. I want to thank President Killeen and everybody at the U of I System for making it possible for me to be here today. And I am deeply honored at the Paul Douglas Award that is being given to me. He is somebody who set the path for so much outstanding public service here in Illinois. Now, I want to start by addressing the elephant in the room. I know people are still wondering why I didn’t speak at the commencement.  \"\"\"\n",
    "dataset = nltk.sent_tokenize(text)\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i] = dataset[i].lower()\n",
    "    dataset[i] = re.sub(r'\\W', ' ', dataset[i])\n",
    "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b6c05fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beans ', 'i was trying to explain to somebody as we were flying in that s corn ', 'that s beans ', 'and they were very impressed at my agricultural knowledge ', 'please give it up for amaury once again for that outstanding introduction ', 'i have a bunch of good friends here today including somebody who i served with who is one of the finest senators in the country and we re lucky to have him your senator dick durbin is here ', 'i also noticed by the way former governor edgar here who i haven t seen in a long time and somehow he has not aged and i have ', 'and it s great to see you governor ', 'i want to thank president killeen and everybody at the u of i system for making it possible for me to be here today ', 'and i am deeply honored at the paul douglas award that is being given to me ', 'he is somebody who set the path for so much outstanding public service here in illinois ', 'now i want to start by addressing the elephant in the room ', 'i know people are still wondering why i didn t speak at the commencement ']\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07830bff",
   "metadata": {},
   "source": [
    "Step 2: Obtaining most frequent words in our text.\n",
    "\n",
    "We will apply the following steps to generate our model.\n",
    "\n",
    "1)We declare a dictionary to hold our bag of words.\n",
    "2)Next we tokenize each sentence to words.\n",
    "3)Now for each word in sentence, we check if the word exists in our dictionary.\n",
    "If it does, then we increment its count by 1. If it doesn’t, we add it to our dictionary and set its count as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa20acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model\n",
    "word2count = {}\n",
    "for data in dataset:\n",
    "    words = nltk.word_tokenize(data)\n",
    "    for word in words:\n",
    "        if word not in word2count.keys():\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6d0a5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beans': 2, 'i': 12, 'was': 1, 'trying': 1, 'to': 8, 'explain': 1, 'somebody': 3, 'as': 1, 'we': 2, 'were': 2, 'flying': 1, 'in': 5, 'that': 4, 's': 3, 'corn': 1, 'and': 7, 'they': 1, 'very': 1, 'impressed': 1, 'at': 4, 'my': 1, 'agricultural': 1, 'knowledge': 1, 'please': 1, 'give': 1, 'it': 3, 'up': 1, 'for': 5, 'amaury': 1, 'once': 1, 'again': 1, 'outstanding': 2, 'introduction': 1, 'have': 3, 'a': 2, 'bunch': 1, 'of': 3, 'good': 1, 'friends': 1, 'here': 5, 'today': 2, 'including': 1, 'who': 4, 'served': 1, 'with': 1, 'is': 4, 'one': 1, 'the': 9, 'finest': 1, 'senators': 1, 'country': 1, 're': 1, 'lucky': 1, 'him': 1, 'your': 1, 'senator': 1, 'dick': 1, 'durbin': 1, 'also': 1, 'noticed': 1, 'by': 2, 'way': 1, 'former': 1, 'governor': 2, 'edgar': 1, 'haven': 1, 't': 2, 'seen': 1, 'long': 1, 'time': 1, 'somehow': 1, 'he': 2, 'has': 1, 'not': 1, 'aged': 1, 'great': 1, 'see': 1, 'you': 1, 'want': 2, 'thank': 1, 'president': 1, 'killeen': 1, 'everybody': 1, 'u': 1, 'system': 1, 'making': 1, 'possible': 1, 'me': 2, 'be': 1, 'am': 1, 'deeply': 1, 'honored': 1, 'paul': 1, 'douglas': 1, 'award': 1, 'being': 1, 'given': 1, 'set': 1, 'path': 1, 'so': 1, 'much': 1, 'public': 1, 'service': 1, 'illinois': 1, 'now': 1, 'start': 1, 'addressing': 1, 'elephant': 1, 'room': 1, 'know': 1, 'people': 1, 'are': 1, 'still': 1, 'wondering': 1, 'why': 1, 'didn': 1, 'speak': 1, 'commencement': 1}\n"
     ]
    }
   ],
   "source": [
    "print(word2count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627b84d7",
   "metadata": {},
   "source": [
    "when processing large texts, the number of words could reach millions. We do not need to use all those words. Hence, we select a particular number of most frequently used words. To implement this we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c0acf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "freq_words = heapq.nlargest(100, word2count, key=word2count.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1714e783",
   "metadata": {},
   "source": [
    "Step: Building the Bag of Words model\n",
    "In this step we construct a vector, which would tell us whether a word in each sentence is a frequent word or not. If a word in a sentence is a frequent word, we set it as 1, else we set it as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "946c360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "numOfWordsA={}\n",
    "for data in dataset:\n",
    "    vector = []\n",
    "    for word in freq_words:\n",
    "        if word in nltk.word_tokenize(data):\n",
    "            vector.append(1)\n",
    "            numOfWordsA[word] = 1\n",
    "        else:\n",
    "            vector.append(0)\n",
    "            numOfWordsA[word] = 0\n",
    "    X.append(vector)\n",
    "X = np.asarray(X)\n",
    "df= pd.DataFrame([numOfWordsA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c0bda4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [1 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 0 ... 1 1 1]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18899207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   i  the  to  and  in  for  here  that  at  who  ...  deeply  honored  paul  \\\n",
      "0  1    1   0    0   0    0     0     0   1    0  ...       0        0     0   \n",
      "\n",
      "   douglas  award  being  given  set  path  so  \n",
      "0        0      0      0      0    0     0   0  \n",
      "\n",
      "[1 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
